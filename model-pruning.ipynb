{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Pruning\n",
      "+-----------------------------------------+------------------+\n",
      "|                  Layer                  | Total Parameters |\n",
      "+-----------------------------------------+------------------+\n",
      "|    self_attn.0.attention.fc_q.weight    |      262144      |\n",
      "|     self_attn.0.attention.fc_q.bias     |       512        |\n",
      "|    self_attn.0.attention.fc_k.weight    |      262144      |\n",
      "|     self_attn.0.attention.fc_k.bias     |       512        |\n",
      "|    self_attn.0.attention.fc_v.weight    |      262144      |\n",
      "|     self_attn.0.attention.fc_v.bias     |       512        |\n",
      "|    self_attn.0.attention.fc_o.weight    |      262144      |\n",
      "|     self_attn.0.attention.fc_o.bias     |       512        |\n",
      "|      self_attn.0.layer_norm.weight      |       512        |\n",
      "|       self_attn.0.layer_norm.bias       |       512        |\n",
      "|    cross_attn.0.attention.fc_q.weight   |      262144      |\n",
      "|     cross_attn.0.attention.fc_q.bias    |       512        |\n",
      "|    cross_attn.0.attention.fc_k.weight   |      262144      |\n",
      "|     cross_attn.0.attention.fc_k.bias    |       512        |\n",
      "|    cross_attn.0.attention.fc_v.weight   |      262144      |\n",
      "|     cross_attn.0.attention.fc_v.bias    |       512        |\n",
      "|    cross_attn.0.attention.fc_o.weight   |      262144      |\n",
      "|     cross_attn.0.attention.fc_o.bias    |       512        |\n",
      "|      cross_attn.0.layer_norm.weight     |       512        |\n",
      "|       cross_attn.0.layer_norm.bias      |       512        |\n",
      "|  cross_attn_rel.0.attention.fc_q.weight |      262144      |\n",
      "|   cross_attn_rel.0.attention.fc_q.bias  |       512        |\n",
      "|  cross_attn_rel.0.attention.fc_k.weight |      262144      |\n",
      "|   cross_attn_rel.0.attention.fc_k.bias  |       512        |\n",
      "|  cross_attn_rel.0.attention.fc_v.weight |      262144      |\n",
      "|   cross_attn_rel.0.attention.fc_v.bias  |       512        |\n",
      "|  cross_attn_rel.0.attention.fc_o.weight |      262144      |\n",
      "|   cross_attn_rel.0.attention.fc_o.bias  |       512        |\n",
      "|    cross_attn_rel.0.layer_norm.weight   |       512        |\n",
      "|     cross_attn_rel.0.layer_norm.bias    |       512        |\n",
      "|   gcn_2ds.0.edgeatten.nn_edge.0.weight  |     1572864      |\n",
      "|    gcn_2ds.0.edgeatten.nn_edge.0.bias   |       1024       |\n",
      "|   gcn_2ds.0.edgeatten.nn_edge.2.weight  |      524288      |\n",
      "|    gcn_2ds.0.edgeatten.nn_edge.2.bias   |       512        |\n",
      "|     gcn_2ds.0.edgeatten.nn.0.weight     |      16384       |\n",
      "|      gcn_2ds.0.edgeatten.nn.0.bias      |       128        |\n",
      "|     gcn_2ds.0.edgeatten.nn.3.weight     |       4096       |\n",
      "|      gcn_2ds.0.edgeatten.nn.3.bias      |        32        |\n",
      "|  gcn_2ds.0.edgeatten.proj_edge.0.weight |      262144      |\n",
      "|   gcn_2ds.0.edgeatten.proj_edge.0.bias  |       512        |\n",
      "| gcn_2ds.0.edgeatten.proj_query.0.weight |      262144      |\n",
      "|  gcn_2ds.0.edgeatten.proj_query.0.bias  |       512        |\n",
      "| gcn_2ds.0.edgeatten.proj_value.0.weight |      131072      |\n",
      "|  gcn_2ds.0.edgeatten.proj_value.0.bias  |       256        |\n",
      "|         gcn_2ds.0.prop.0.weight         |      589824      |\n",
      "|          gcn_2ds.0.prop.0.bias          |       768        |\n",
      "|         gcn_2ds.0.prop.2.weight         |      393216      |\n",
      "|          gcn_2ds.0.prop.2.bias          |       512        |\n",
      "|   gcn_3ds.0.edgeatten.nn_edge.0.weight  |     1572864      |\n",
      "|    gcn_3ds.0.edgeatten.nn_edge.0.bias   |       1024       |\n",
      "|   gcn_3ds.0.edgeatten.nn_edge.2.weight  |      524288      |\n",
      "|    gcn_3ds.0.edgeatten.nn_edge.2.bias   |       512        |\n",
      "|     gcn_3ds.0.edgeatten.nn.0.weight     |      16384       |\n",
      "|      gcn_3ds.0.edgeatten.nn.0.bias      |       128        |\n",
      "|     gcn_3ds.0.edgeatten.nn.3.weight     |       4096       |\n",
      "|      gcn_3ds.0.edgeatten.nn.3.bias      |        32        |\n",
      "|  gcn_3ds.0.edgeatten.proj_edge.0.weight |      262144      |\n",
      "|   gcn_3ds.0.edgeatten.proj_edge.0.bias  |       512        |\n",
      "| gcn_3ds.0.edgeatten.proj_query.0.weight |      262144      |\n",
      "|  gcn_3ds.0.edgeatten.proj_query.0.bias  |       512        |\n",
      "| gcn_3ds.0.edgeatten.proj_value.0.weight |      131072      |\n",
      "|  gcn_3ds.0.edgeatten.proj_value.0.bias  |       256        |\n",
      "|         gcn_3ds.0.prop.0.weight         |      589824      |\n",
      "|          gcn_3ds.0.prop.0.bias          |       768        |\n",
      "|         gcn_3ds.0.prop.2.weight         |      393216      |\n",
      "|          gcn_3ds.0.prop.2.bias          |       512        |\n",
      "|          self_attn_fc.0.weight          |       128        |\n",
      "|           self_attn_fc.0.bias           |        32        |\n",
      "|          self_attn_fc.2.weight          |        32        |\n",
      "|           self_attn_fc.2.bias           |        32        |\n",
      "|          self_attn_fc.3.weight          |       1024       |\n",
      "|           self_attn_fc.3.bias           |        32        |\n",
      "|          self_attn_fc.5.weight          |        32        |\n",
      "|           self_attn_fc.5.bias           |        32        |\n",
      "|          self_attn_fc.6.weight          |       256        |\n",
      "|           self_attn_fc.6.bias           |        8         |\n",
      "+-----------------------------------------+------------------+\n",
      "Total Parameters: 10677128\n",
      "After Pruning\n",
      "+---------------------------------------+------------------+\n",
      "|                 Layer                 | Total Parameters |\n",
      "+---------------------------------------+------------------+\n",
      "|    self_attn.0.attention.fc_q.bias    |       512        |\n",
      "|    self_attn.0.attention.fc_k.bias    |       512        |\n",
      "|    self_attn.0.attention.fc_v.bias    |       512        |\n",
      "|    self_attn.0.attention.fc_o.bias    |       512        |\n",
      "|     self_attn.0.layer_norm.weight     |       512        |\n",
      "|      self_attn.0.layer_norm.bias      |       512        |\n",
      "|    cross_attn.0.attention.fc_q.bias   |       512        |\n",
      "|    cross_attn.0.attention.fc_k.bias   |       512        |\n",
      "|    cross_attn.0.attention.fc_v.bias   |       512        |\n",
      "|    cross_attn.0.attention.fc_o.bias   |       512        |\n",
      "|     cross_attn.0.layer_norm.weight    |       512        |\n",
      "|      cross_attn.0.layer_norm.bias     |       512        |\n",
      "|  cross_attn_rel.0.attention.fc_q.bias |       512        |\n",
      "|  cross_attn_rel.0.attention.fc_k.bias |       512        |\n",
      "|  cross_attn_rel.0.attention.fc_v.bias |       512        |\n",
      "|  cross_attn_rel.0.attention.fc_o.bias |       512        |\n",
      "|   cross_attn_rel.0.layer_norm.weight  |       512        |\n",
      "|    cross_attn_rel.0.layer_norm.bias   |       512        |\n",
      "|   gcn_2ds.0.edgeatten.nn_edge.0.bias  |       1024       |\n",
      "|   gcn_2ds.0.edgeatten.nn_edge.2.bias  |       512        |\n",
      "|    gcn_2ds.0.edgeatten.nn.0.weight    |      16384       |\n",
      "|     gcn_2ds.0.edgeatten.nn.0.bias     |       128        |\n",
      "|    gcn_2ds.0.edgeatten.nn.3.weight    |       4096       |\n",
      "|     gcn_2ds.0.edgeatten.nn.3.bias     |        32        |\n",
      "|  gcn_2ds.0.edgeatten.proj_edge.0.bias |       512        |\n",
      "| gcn_2ds.0.edgeatten.proj_query.0.bias |       512        |\n",
      "| gcn_2ds.0.edgeatten.proj_value.0.bias |       256        |\n",
      "|         gcn_2ds.0.prop.0.bias         |       768        |\n",
      "|         gcn_2ds.0.prop.2.bias         |       512        |\n",
      "|   gcn_3ds.0.edgeatten.nn_edge.0.bias  |       1024       |\n",
      "|   gcn_3ds.0.edgeatten.nn_edge.2.bias  |       512        |\n",
      "|    gcn_3ds.0.edgeatten.nn.0.weight    |      16384       |\n",
      "|     gcn_3ds.0.edgeatten.nn.0.bias     |       128        |\n",
      "|    gcn_3ds.0.edgeatten.nn.3.weight    |       4096       |\n",
      "|     gcn_3ds.0.edgeatten.nn.3.bias     |        32        |\n",
      "|  gcn_3ds.0.edgeatten.proj_edge.0.bias |       512        |\n",
      "| gcn_3ds.0.edgeatten.proj_query.0.bias |       512        |\n",
      "| gcn_3ds.0.edgeatten.proj_value.0.bias |       256        |\n",
      "|         gcn_3ds.0.prop.0.bias         |       768        |\n",
      "|         gcn_3ds.0.prop.2.bias         |       512        |\n",
      "|          self_attn_fc.0.bias          |        32        |\n",
      "|         self_attn_fc.2.weight         |        32        |\n",
      "|          self_attn_fc.2.bias          |        32        |\n",
      "|          self_attn_fc.3.bias          |        32        |\n",
      "|         self_attn_fc.5.weight         |        32        |\n",
      "|          self_attn_fc.5.bias          |        32        |\n",
      "|          self_attn_fc.6.bias          |        8         |\n",
      "+---------------------------------------+------------------+\n",
      "Total Parameters: 58888\n",
      "Reduction in parameters: 10618240 (99.45%)\n"
     ]
    }
   ],
   "source": [
    "# !pip install prettytable\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src.model.model_utils.network_util import (MLP, Aggre_Index, Gen_Index,\n",
    "                                                build_mlp)\n",
    "from src.model.transformer.attention import MultiHeadAttention\n",
    "\n",
    "\n",
    "class GraphEdgeAttenNetwork(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads, dim_node, dim_edge, dim_atten, aggr= 'max', use_bn=False,\n",
    "                 flow='target_to_source',attention = 'fat',use_edge:bool=True, **kwargs):\n",
    "        super().__init__() #  \"Max\" aggregation.\n",
    "        self.name = 'edgeatten'\n",
    "        self.dim_node=dim_node\n",
    "        self.dim_edge=dim_edge\n",
    "        self.index_get = Gen_Index(flow=flow)\n",
    "        if attention == 'fat':        \n",
    "            self.index_aggr = Aggre_Index(aggr=aggr,flow=flow)\n",
    "        elif attention == 'distance':\n",
    "            aggr = 'add'\n",
    "            self.index_aggr = Aggre_Index(aggr=aggr,flow=flow)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        self.edgeatten = MultiHeadedEdgeAttention(\n",
    "            dim_node=dim_node,dim_edge=dim_edge,dim_atten=dim_atten,\n",
    "            num_heads=num_heads,use_bn=use_bn,attention=attention,use_edge=use_edge, **kwargs)\n",
    "        self.prop = build_mlp([dim_node+dim_atten, dim_node+dim_atten, dim_node],\n",
    "                            do_bn= use_bn, on_last=False)\n",
    "\n",
    "    def forward(self, x, edge_feature, edge_index, weight=None, istrain=False):\n",
    "        assert x.ndim == 2\n",
    "        assert edge_feature.ndim == 2\n",
    "        x_i, x_j = self.index_get(x, edge_index)\n",
    "        xx, gcn_edge_feature, prob = self.edgeatten(x_i, edge_feature, x_j, weight, istrain=istrain)\n",
    "        xx = self.index_aggr(xx, edge_index, dim_size = x.shape[0])\n",
    "        xx = self.prop(torch.cat([x,xx],dim=1))\n",
    "        return xx, gcn_edge_feature\n",
    "class MultiHeadedEdgeAttention(torch.nn.Module):\n",
    "    def __init__(self, num_heads: int, dim_node: int, dim_edge: int, dim_atten: int, use_bn=False,\n",
    "                 attention = 'fat', use_edge:bool = True, **kwargs):\n",
    "        super().__init__()\n",
    "        assert dim_node % num_heads == 0\n",
    "        assert dim_edge % num_heads == 0\n",
    "        assert dim_atten % num_heads == 0\n",
    "        self.name = 'MultiHeadedEdgeAttention'\n",
    "        self.dim_node=dim_node\n",
    "        self.dim_edge=dim_edge\n",
    "        self.d_n = d_n = dim_node // num_heads\n",
    "        self.d_e = d_e = dim_edge // num_heads\n",
    "        self.d_o = d_o = dim_atten // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.use_edge = use_edge\n",
    "        self.nn_edge = build_mlp([dim_node*2+dim_edge,(dim_node+dim_edge),dim_edge],\n",
    "                          do_bn= use_bn, on_last=False)\n",
    "        self.mask_obj = 0.5\n",
    "        \n",
    "        DROP_OUT_ATTEN = None\n",
    "        if 'DROP_OUT_ATTEN' in kwargs:\n",
    "            DROP_OUT_ATTEN = kwargs['DROP_OUT_ATTEN']\n",
    "            # print('drop out in',self.name,'with value',DROP_OUT_ATTEN)\n",
    "        \n",
    "        self.attention = attention\n",
    "        assert self.attention in ['fat']\n",
    "        \n",
    "        if self.attention == 'fat':\n",
    "            if use_edge:\n",
    "                self.nn = MLP([d_n+d_e, d_n+d_e, d_o],do_bn=use_bn,drop_out = DROP_OUT_ATTEN)\n",
    "            else:\n",
    "                self.nn = MLP([d_n, d_n*2, d_o],do_bn=use_bn,drop_out = DROP_OUT_ATTEN)\n",
    "                \n",
    "            self.proj_edge  = build_mlp([dim_edge,dim_edge])\n",
    "            self.proj_query = build_mlp([dim_node,dim_node])\n",
    "            self.proj_value = build_mlp([dim_node,dim_atten])\n",
    "        elif self.attention == 'distance':\n",
    "            self.proj_value = build_mlp([dim_node,dim_atten])\n",
    "\n",
    "        \n",
    "    def forward(self, query, edge, value, weight=None, istrain=False):\n",
    "        batch_dim = query.size(0)\n",
    "        \n",
    "        edge_feature = torch.cat([query, edge, value],dim=1)\n",
    "        # avoid overfitting by mask relation input object feature\n",
    "        # if random.random() < self.mask_obj and istrain: \n",
    "        #     feat_mask = torch.cat([torch.ones_like(query),torch.zeros_like(edge), torch.ones_like(value)],dim=1)\n",
    "        #     edge_feature = torch.where(feat_mask == 1, edge_feature, torch.zeros_like(edge_feature))\n",
    "        \n",
    "        edge_feature = self.nn_edge( edge_feature )#.view(b, -1, 1)\n",
    "\n",
    "        if self.attention == 'fat':\n",
    "            value = self.proj_value(value)\n",
    "            query = self.proj_query(query).view(batch_dim, self.d_n, self.num_heads)\n",
    "            edge = self.proj_edge(edge).view(batch_dim, self.d_e, self.num_heads)\n",
    "            if self.use_edge:\n",
    "                prob = self.nn(torch.cat([query,edge],dim=1)) # b, dim, head    \n",
    "            else:\n",
    "                prob = self.nn(query) # b, dim, head \n",
    "            prob = prob.softmax(1)\n",
    "            x = torch.einsum('bm,bm->bm', prob.reshape_as(value), value)\n",
    "        \n",
    "        elif self.attention == 'distance':\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError('')\n",
    "        \n",
    "        return x, edge_feature, prob\n",
    "\n",
    "class MMG(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim_node, dim_edge, dim_atten, num_heads=1, aggr= 'max', \n",
    "                 use_bn=False,flow='target_to_source', attention = 'fat', \n",
    "                 hidden_size=512, depth=1, use_edge:bool=True, **kwargs,\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = depth\n",
    "\n",
    "        self.self_attn = nn.ModuleList(\n",
    "            MultiHeadAttention(d_model=dim_node, d_k=dim_node // num_heads, d_v=dim_node // num_heads, h=num_heads) for i in range(depth))\n",
    "\n",
    "        self.cross_attn = nn.ModuleList(\n",
    "            MultiHeadAttention(d_model=dim_node, d_k=dim_node // num_heads, d_v=dim_node // num_heads, h=num_heads) for i in range(depth))\n",
    "\n",
    "        self.cross_attn_rel = nn.ModuleList(\n",
    "            MultiHeadAttention(d_model=dim_edge, d_k=dim_edge // num_heads, d_v=dim_edge // num_heads, h=num_heads) for i in range(depth))\n",
    "        \n",
    "        self.gcn_2ds = torch.nn.ModuleList()\n",
    "        self.gcn_3ds = torch.nn.ModuleList()\n",
    "        \n",
    "        for _ in range(self.depth):\n",
    "\n",
    "            self.gcn_2ds.append(GraphEdgeAttenNetwork(\n",
    "                            num_heads,\n",
    "                            dim_node,\n",
    "                            dim_edge,\n",
    "                            dim_atten,\n",
    "                            aggr,\n",
    "                            use_bn=use_bn,\n",
    "                            flow=flow,\n",
    "                            attention=attention,\n",
    "                            use_edge=use_edge, \n",
    "                            **kwargs))\n",
    "            \n",
    "            self.gcn_3ds.append(GraphEdgeAttenNetwork(\n",
    "                            num_heads,\n",
    "                            dim_node,\n",
    "                            dim_edge,\n",
    "                            dim_atten,\n",
    "                            aggr,\n",
    "                            use_bn=use_bn,\n",
    "                            flow=flow,\n",
    "                            attention=attention,\n",
    "                            use_edge=use_edge, \n",
    "                            **kwargs))\n",
    "           \n",
    "        self.self_attn_fc = nn.Sequential(  # 11 32 32 4(head)\n",
    "            nn.Linear(4, 32),  # xyz, dist\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(32),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(32),\n",
    "            nn.Linear(32, num_heads)\n",
    "        )\n",
    "        \n",
    "        self.drop_out = torch.nn.Dropout(kwargs['DROP_OUT_ATTEN'])\n",
    "    \n",
    "    \n",
    "    def forward(self, obj_feature_3d, obj_feature_2d, edge_feature_3d, edge_feature_2d, edge_index, batch_ids, obj_center=None, discriptor=None, istrain=False):\n",
    "\n",
    "        # compute weight for obj\n",
    "        if obj_center is not None:\n",
    "            # get attention weight for object\n",
    "            batch_size = batch_ids.max().item() + 1\n",
    "            N_K = obj_feature_3d.shape[0]\n",
    "            obj_mask = torch.zeros(1, 1, N_K, N_K).cuda()\n",
    "            obj_distance_weight = torch.zeros(1, self.num_heads, N_K, N_K).cuda()\n",
    "            count = 0\n",
    "\n",
    "            for i in range(batch_size):\n",
    "\n",
    "                idx_i = torch.where(batch_ids == i)[0]\n",
    "                obj_mask[:, :, count:count + len(idx_i), count:count + len(idx_i)] = 1\n",
    "            \n",
    "                center_A = obj_center[None, idx_i, :].clone().detach().repeat(len(idx_i), 1, 1)\n",
    "                center_B = obj_center[idx_i, None, :].clone().detach().repeat(1, len(idx_i), 1)\n",
    "                center_dist = (center_A - center_B)\n",
    "                dist = center_dist.pow(2)\n",
    "                dist = torch.sqrt(torch.sum(dist, dim=-1))[:, :, None]\n",
    "                weights = torch.cat([center_dist, dist], dim=-1).unsqueeze(0)  # 1 N N 4\n",
    "                dist_weights = self.self_attn_fc(weights).permute(0,3,1,2)  # 1 num_heads N N\n",
    "                \n",
    "                attention_matrix_way = 'add'\n",
    "                obj_distance_weight[:, :, count:count + len(idx_i), count:count + len(idx_i)] = dist_weights\n",
    "\n",
    "                count += len(idx_i)\n",
    "        else:\n",
    "            obj_mask = None\n",
    "            obj_distance = None\n",
    "            attention_matrix_way = 'mul'\n",
    "\n",
    "\n",
    "        for i in range(self.depth):\n",
    "\n",
    "            obj_feature_3d = obj_feature_3d.unsqueeze(0)\n",
    "            obj_feature_2d = obj_feature_2d.unsqueeze(0)\n",
    "            \n",
    "            obj_feature_3d = self.self_attn[i](obj_feature_3d, obj_feature_3d, obj_feature_3d, attention_weights=obj_distance_weight, way=attention_matrix_way, attention_mask=obj_mask, use_knn=False)\n",
    "            obj_feature_2d = self.cross_attn[i](obj_feature_2d, obj_feature_3d, obj_feature_3d, attention_weights=obj_distance_weight, way=attention_matrix_way, attention_mask=obj_mask, use_knn=False)\n",
    "            \n",
    "            obj_feature_3d = obj_feature_3d.squeeze(0)\n",
    "            obj_feature_2d = obj_feature_2d.squeeze(0)  \n",
    "\n",
    "\n",
    "            obj_feature_3d, edge_feature_3d = self.gcn_3ds[i](obj_feature_3d, edge_feature_3d, edge_index, istrain=istrain)\n",
    "            obj_feature_2d, edge_feature_2d = self.gcn_2ds[i](obj_feature_2d, edge_feature_2d, edge_index, istrain=istrain)\n",
    "\n",
    "            \n",
    "            edge_feature_2d = edge_feature_2d.unsqueeze(0)\n",
    "            edge_feature_3d = edge_feature_3d.unsqueeze(0)\n",
    "            \n",
    "            edge_feature_2d = self.cross_attn_rel[i](edge_feature_2d, edge_feature_3d, edge_feature_3d, use_knn=False)\n",
    "            \n",
    "            edge_feature_2d = edge_feature_2d.squeeze(0)\n",
    "            edge_feature_3d = edge_feature_3d.squeeze(0)\n",
    "\n",
    "            if i < (self.depth-1) or self.depth==1:\n",
    "                \n",
    "                obj_feature_3d = F.relu(obj_feature_3d)\n",
    "                obj_feature_3d = self.drop_out(obj_feature_3d)\n",
    "                \n",
    "                obj_feature_2d = F.relu(obj_feature_2d)\n",
    "                obj_feature_2d = self.drop_out(obj_feature_2d)\n",
    "\n",
    "                edge_feature_3d = F.relu(edge_feature_3d)\n",
    "                edge_feature_3d = self.drop_out(edge_feature_3d)\n",
    "\n",
    "                edge_feature_2d = F.relu(edge_feature_2d)\n",
    "                edge_feature_2d = self.drop_out(edge_feature_2d)\n",
    "        \n",
    "        return obj_feature_3d, obj_feature_2d, edge_feature_3d, edge_feature_2d\n",
    "\n",
    "def print_parameters(model, title=\"Model Parameters\"):\n",
    "    table = PrettyTable([\"Layer\", \"Total Parameters\"])\n",
    "    total_params = 0\n",
    "\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        param_total = parameter.numel()\n",
    "        if any(substring in name for substring in ['_orig', '_mask']):\n",
    "            continue\n",
    "        table.add_row([name, param_total])\n",
    "        total_params += param_total\n",
    "    \n",
    "    print(title)\n",
    "    print(table)\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    return total_params  \n",
    "\n",
    "\n",
    "def prune_model(model, pruning_rate=0.2):\n",
    "    parameters_to_prune = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            parameters_to_prune.append((module, 'weight'))\n",
    "    \n",
    "    prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=pruning_rate,\n",
    "    )\n",
    "\n",
    "dim_node = 512\n",
    "dim_edge = 512      \n",
    "dim_atten = 256 \n",
    "num_heads = 8  \n",
    "drop_out_atten = 0.1\n",
    "model = MMG(dim_node, dim_edge, dim_atten, num_heads, DROP_OUT_ATTEN=drop_out_atten)\n",
    "\n",
    "before_pruning_params = print_parameters(model, \"Before Pruning\")\n",
    "prune_model(model, pruning_rate=0.5)\n",
    "after_pruning_params = print_parameters(model, \"After Pruning\")\n",
    "\n",
    "# Calculate and print the reduction in parameters\n",
    "reduction = before_pruning_params - after_pruning_params\n",
    "print(f\"Reduction in parameters: {reduction} ({(reduction / before_pruning_params) * 100:.2f}%)\")\n",
    "\n",
    "# report_content = f\"\"\"\n",
    "# Before Pruning: {before_pruning_params} parameters\n",
    "# After Pruning: {after_pruning_params} parameters\n",
    "# Reduction in parameters: {reduction} ({(reduction / before_pruning_params) * 100:.2f}%)\n",
    "# \"\"\"\n",
    "\n",
    "# # 파일에 결과 저장\n",
    "# with open(\"mmg_pruning.txt\", \"w\") as file:\n",
    "#     file.write(report_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from src.model.model_utils.network_util import build_mlp, Gen_Index, Aggre_Index, MLP\n",
    "from src.model.model_utils.networks_base import BaseNetwork\n",
    "from src.model.transformer.attention import MultiHeadAttention\n",
    "import inspect\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "from src.utils import op_utils\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def apply_pruning(model, pruning_rate=0.5, save_path=\"pruning.txt\"):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=pruning_rate)\n",
    "            prune.remove(module, 'weight')\n",
    "            \n",
    "    table = PrettyTable([\"Layer\", \"Total Parameters\", \"Non-zero Parameters\", \"Sparsity (%)\"])\n",
    "    total_params = total_non_zero = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        non_zero_params = torch.count_nonzero(param).item()\n",
    "        sparsity = 100.0 * (1 - non_zero_params / num_params)\n",
    "        table.add_row([name, num_params, non_zero_params, f\"{sparsity:.2f}\"])\n",
    "        total_params += num_params\n",
    "        total_non_zero += non_zero_params\n",
    "    \n",
    "    total_sparsity = 100.0 * (1 - total_non_zero / total_params)\n",
    "    table.add_row([\"Total\", total_params, total_non_zero, f\"{total_sparsity:.2f}\"])\n",
    "    \n",
    "    print(apply_pruning)\n",
    "    # with open(save_path, \"w\") as f:\n",
    "    #     f.write(str(table))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlsat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
